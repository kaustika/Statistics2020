\documentclass[../body.tex]{subfiles}
\begin{document}
\subsection{Метод максимального правдоподобия}
Одним из универсальных методов оценивания является метод максимального правдоподобия, предложенный Р.Фишером $(1921)$.\\ \\
Пусть $x_{1},...,x_{n}$ — случайная выборка из генеральной совокупности с плотностью вероятности $f(x,\theta$); $L(x_{1},... ,x_{n}, \theta)$ — функция правдоподобия (ФП), представляющая собой совместную плотность вероятности независимых с.в. $x_{1}, ... ,x_{n}$ и рассматриваемая как функция неизвестного параметра $\theta$:
\begin{equation}
	L(x_{1},...,x_{n},\theta) = f(x_{1},\theta)f(x_{2},\theta)...f(x_{n}, \theta)
	\label{L()}
\end{equation}
\textbf{Определение}. Оценкой максимального правдоподобия (о.м.п.) будем называть такое значение $\hat{\theta}_{ml}$ из множества допустимых значений параметра $\theta$, для которого ФП принимает наибольшее значение при заданных $x_{1},...,x_{n}$:
\begin{equation}
	\hat{\theta}_{ml} = \arg \max_{\theta}L(x_{1},...,x_{n},\theta)
	\label{theta_mp}
\end{equation}
Если ФП дважды дифференцируема, то её стационарные значения даются корнями уравнения
\begin{equation}
	\frac{\partial L(x_{1},...,x_{n},\theta)}{\partial \theta} = 0
	\label{eq_min}
\end{equation}
Достаточным условием того, чтобы некоторое стационарное значениe $\Tilde{\theta}$ было локальным максимумом, является неравенство
\begin{equation}
	\frac{\partial^{2}L}{\partial \theta^{2}}(x_{1},...,x_{n},\Tilde{\theta}) < 0
	\label{ineq_min}
\end{equation}
Определив точки локальных максимумов ФП (если их несколько), находят наибольший, который и даёт решение задачи (\ref{L()}).
Часто проще искать максимум логарифма ФП, так как он имеет максимум в одной точке с ФП:
\begin{equation}
	\frac{\partial \ln L}{\partial \theta}=\frac{1}{L}\frac{\partial L}{\partial \theta}, если L > 0
	\label{log_max}
\end{equation}
и соответственно решать уравнение
\begin{equation}
	\frac{\partial \ln L}{\partial \theta}= 0
	\label{log_m=0}
\end{equation}
которое называют \textit{уравнением правдоподобия}.
В задаче оценивания векторного параметра $\theta = (\theta_{1}, ... ,\theta_{m})$ аналогично (\ref{theta_mp}) находится максимум ФП нескольких аргументов: 
\begin{equation}
	\hat{\theta_{мп}} = \arg \max_{\theta_{1}, \theta_{2}...\theta_{m}} L(x_{1}, x_{2},...x_{n}, \theta_{1}, \theta_{2},...\theta_{m})
	\label{multi_theta}
\end{equation}
и в случае дифференцируемости ФП выписывается система уравнений правдоподобия
\begin{equation}
	\frac{\partial L}{\partial \theta_{k}} = 0 \text{  или  } \frac{\partial \ln L}{\partial \theta_{k}} = 0, k = 1,..m
\end{equation}

\textbf{Пример:} \label{normal} Вывод оценок математического ожидания $\mu$ и дисперсии $\sigma^2$ нормального распределения $N(\mu, \sigma^2)$.
Для этого составим функцию правдоподибия:
$$L(x_1, ..., x_n, \mu, \sigma^2) = \prod\limits_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}}exp\left[-\frac{(x_i - \mu)^2}{2\sigma}\right] = (2\pi\sigma^2)^{-n/2}exp\left[-\frac{1}{2\sigma^2}\sum_{i = 1}^{n}(x_i - \mu)^2\right],$$

$$\ln L = -\frac{n}{2}\ln 2\pi - \frac{1}{2}\ln \sigma^2 - \frac{1}{2\sigma^2}\sum_{i = 1}^{n}(x_i - \mu)^2.$$
Получим такие уравнения правдоподобия:
$$
\begin{cases}
\frac{\partial\ln L}{\ln \mu} = \frac{1}{\hat{\sigma}^2} \sum_{i = 1}^{n}(x_i - \hat{\mu}) = \frac{n}{\hat{\sigma}^2}(\overline{x} - \hat{\mu}) = 0,\\
\frac{\partial \ln L}{\partial (\sigma^2)} = -\frac{n}{2\hat{\sigma}^2} + \frac{1}{2(\hat{\sigma}^2)^2}\sum_{i = 1}^{n}(x_i - \mu)^2 = \frac{n}{2(\hat{\sigma}^2)^2}\left[\frac{1}{n}\sum_{i = 1}^{n}(x_i - \hat{\mu})^2 - \hat{\sigma}^2\right] = 0,
\end{cases}
$$
откуда следует, что:
\begin{itemize}
	\item Выборочное среднее $\overline{x}$ - оценка максимального правдоподобия математического ожидания: $\hat{\mu}_{ml} = \overline{x}$,
	\item Выборочная дисперсия $s^2 = \frac{1}{n}\sum_{i = 1}^{n}(x_i - \overline{x})^2$ - оценка максимального правдоподобия генеральной дисперсии: $\hat{\sigma}_{ml}^2 = s^2$\cite[c.~442 - 444]{max}.
\end{itemize}
Эти результаты мы используем при выполнении задания.






\subsection{Проверка гипотезы о законе распределения генеральной совокупности. Метод хи-квадрат}
Исчерпывающей характеристикой изучаемой случайной величины является её закон распределения. Поэтому естественно стремление исследователей построить этот закон приближённо на основе статистических данных.
\\ \\
Сначала выдвигается гипотеза о виде закона распределения.\\\\
После того как выбран вид закона, возникает задача оценивания его параметров и проверки (тестирования) закона в целом.
\\ \\
Для проверки гипотезы о законе распределения применяются критерии согласия. Таких критериев существует много. Мы рассмотрим наиболее обоснованный и наиболее часто используемый в практике — критерий $\chi^{2}$ (хи-квадрат), введённый К.Пирсоном ($1900$ г.) для случая, когда параметры распределения известны. Этот критерий был существенно уточнён Р.Фишером ($1924$ г.), когда параметры распределения оцениваются по выборке, используемой для проверки.
\\ \\
Мы ограничимся рассмотрением случая одномерного распределения.
\\ \\
Итак, выдвинута гипотеза $H_{0}$ о генеральном законе распределения с функцией распределения $F(x)$.
\\ \\
Рассматриваем случай, когда гипотетическая функция распределения $F(x)$ не содержит неизвестных параметров.
\\ \\
Разобьём генеральную совокупность, т.е. множество значений изучаемой случайной величины $X$ на $k$ непересекающихся подмножеств $\Delta_{1},\Delta_{2}, ... ,\Delta_{k}$.
\\\\
Пусть $p_{i} = P(X \in \Delta_{i}), i = 1, ... ,k$. 
\\\\
Если генеральная совокупность — вся вещественная ось, то подмножества $\Delta_{i} = (a_{i-1},a_{i}]$ — полуоткрытые промежутки $(i = 2, ... ,k-1)$. Крайние промежутки будут полубесконечными: $\Delta_{1} = (−\infty,a_{1}], \Delta_{k} = (a_{k-1},+\infty).$ В этом случае $p_{i} = F(a_{i})$$-$$F(a_{i-1}); a_{0} = −\infty, a_{k} = +\infty (i = 1, ... ,k).$
\\\\
Отметим, что $\sum_{i=1}^{k}{p_{i}} = 1$.
Будем предполагать, что все $p_{i} > 0 (i = 1, ... ,k).$
\\\\
Пусть, далее, $n_{1},n_{2}, ... ,n_{k}$ — частоты попадания выборочных элементов в подмножества $\Delta_{1},\Delta_{2}, ... ,\Delta_{k}$ соответственно.
\\\\
В случае справедливости гипотезы $H_{0}$ относительные частоты $n_{i}/n$ при большом n должны быть близки к вероятностям $p_{i} (i = 1, ... ,k)$, поэтому за меру отклонения выборочного распределения от гипотетического с функцией F(x) естественно выбрать величину
\begin{equation}
	Z = \sum_{i = 1}^{k}{c_{i}(\frac{n_{i}}{n} - p_{i})^{2}}, 
	\label{Z}
\end{equation}
где $c_{i}$ — какие-нибудь положительные числа (веса). К.Пирсоном в качестве весов выбраны числа $c_{i} = n/p_{i}$ (i = 1, ... ,k). Тогда получается статистика критерия хи-квадрат К.Пирсона
\begin{equation}
	\chi^{2} = \sum_{i = 1}^{k}{\frac{n}{p_{i}}(\frac{n_{i}}{n} - p_{i})^{2}} = \sum_{i = 1}^{k}{\frac{(n_{i} - np_{i})^{2}}{np_{i}}}, 
	\label{chi_2}
\end{equation}
которая обозначена тем же символом, что и закон распределения хи-квадрат.
\\\\
К.Пирсоном доказана теорема об асимптотическом поведении статистики $\chi^{2}$, указывающая путь её применения.
\\\\
\textbf{Теорема К.Пирсона}. Статистика критерия $\chi^{2}$ асимптотически распределена по закону $\chi^{2}$ с $k-1$ степенями свободы.
\\\\
Это означает, что независимо от вида проверяемого распределения, т.е. функции F(x), выборочная функция распределения статистики $\chi^{2}$ при $n \rightarrow \infty$  стремится к функции распределения случайной величины с плотностью вероятности 
\begin{equation}
	f_{k - 1}(x) = 
	\begin{cases}
	& 0 \text{ , } x  \leq 0  \\ 
	& \frac{1}{2^{\frac{k-1}{2}}\Gamma(\frac{k-1}{2})}x^{\frac{k-3}{2}}e^{-\frac{x}{2}}
	\text{ , } x>0 
	\end{cases}
	\label{f_k-1}
\end{equation}
Для прояснения сущности метода $\chi^{2}$ сделаем ряд замечаний.
\\\\
\textbf{Замечание 1}. Выбор подмножеств $\Delta_{1},\Delta_{2}, ... ,\Delta_{k}$ и их числа k в принципе ничем не регламентируется, так как $n \rightarrow \infty$. Но так как число n хотя и очень большое, но конечное, то k должно быть с ним согласовано. Обычно его берут таким же, как и для построения гистограммы, т.е. можно руководствоваться формулой
\begin{equation}
	k \approx 1.72\sqrt[3]{n}
	\label{k_1}
\end{equation}
или формулой Старджесса
\begin{equation}
	k \approx 1 + 3.3lgn
\end{equation}
При этом, если  $\Delta_{1},\Delta_{2}, ... ,\Delta_{k}$ — промежутки, то их длины удобно сделать равными, за исключением крайних — полубесконечных.
\\\\
\textbf{Замечание 2}. (о числе степеней свободы).
Числом степеней свободы функции (по старой терминологии) называется число её независимых аргументов. Аргументами статистики $\chi^{2}$ являются частоты $n_{1},n_{2}, ... ,n_{k}$. Эти частоты связаны одним равенством $n_{1} + n_{2} + ... + n_{k}  = n$, а в остальном независимы в силу независимости элементов выборки. Таким образом, функция $\chi^{2}$  имеет $k-1$ независимых аргументов: число частот минус одна связь. В силу теоремы Пирсона число степеней свободы статистики $\chi^{2}$  отражается на виде асимптотической плотности $f_{k - 1}(x)$.
\\\\
На основе общей схемы проверки статистических гипотез сформулируем следующее правило.
\\\\
\textbf{Правило проверки гипотезы о законе распределения по методу $\chi^{2}$}.
\begin{enumerate}
	\item Выбираем уровень значимости $\alpha$.
	\item  По таблице \cite[c.~ 358]{math} находим квантиль $\chi^{2}_{1-\alpha}(k - 1)$ распределения хи-квадрат с $k−1$ степенями свободы порядка $1-\alpha$. 
	\item С помощью гипотетической функции распределения F(x) вычисляем вероятности $p_{i} = P (X \in \Delta_{i}), i = 1, ... ,k.$
	\item Находим частоты $n_{i}$ попадания элементов выборки в подмножества $\Delta_{i}, i = 1, ... ,k.$ 
	\item Вычисляем выборочное значение статистики критерия $\chi^{2}$:
	\begin{equation}
	\chi^{2}_{B} =\sum_{i = 1}^{k}{\frac{(n_{i} - np_{i})^{2}}{np_{i}}}.
	\label{chi_B}
	\end{equation}
	\item Сравниваем $\chi^{2}_{B}$ и квантиль $\chi^{2}_{1-\alpha}(k-1)$:
	\begin{itemize}
		\item Если $\chi^{2}_{B}$ < $\chi^{2}_{1-\alpha}$(k $-$ 1), то гипотеза $H_{0}$ на данном этапе проверки принимается. 
		\item Если $\chi^{2}_{B}$ ≥ $\chi^{2}_{1-\alpha}(k -1)$, то гипотеза $H_{0}$ отвергается, выбирается одно из альтернативных распределений, и процедура проверки повторяется.
	\end{itemize}
\end{enumerate}
\textbf{Замечание 3}. Из формулы (\ref{chi_2}) видим, что веса $c_{i} = n/p_{i}$ пропорциональны $n$, т.е. с ростом $n$ увеличиваются. Отсюда следует, что если выдвинутая гипотеза неверна, то относительные частоты $n_{i}/n$ не будут близки к вероятностям $p_{i}$, и с ростом n величина  $\chi^{2}_{B}$  будет увеличиваться. При фиксированном уровне значимости $\alpha$ будет фиксировано пороговое число — квантиль $\chi^{2}_{1-\alpha}(k-1)$, поэтому, увеличивая $n$, мы придём к неравенству $\chi^{2}_{B}$ > $\chi^{2}_{1-\alpha}(k-1)$, т.е. с увеличением объёма выборки неверная гипотеза будет отвергнута.
\newline
Отсюда следует, что при сомнительной ситуации, когда $\chi^{2}_{B} \approx \chi^{2}_{1-\alpha}(k-1)$, можно попытаться увеличить объём выборки (например, в $2$ раза), чтобы требуемое неравенство было более чётким.
\newline
\textbf{Замечание 4}. Теория и практика применения критерия  $\chi^{2}$ указывают, что если для каких-либо подмножеств $\Delta_{i} (i = 1, ... ,k)$ условие $np_{i} \geq 5$ не выполняется, то следует объединить соседние подмножества (промежутки).
\newline
Это условие выдвигается требованием близости величин $\frac{(n_{i} −np_{i})}{\sqrt{np_{i}}}$, квадраты которых являются слагаемыми $\chi^{2}$  к нормальным $N(0,1)$. Тогда случайная величина в формуле (\ref{chi_2}) будет распределена по закону, близкому к хи-квадрат. Такая близость обеспечивается достаточной численностью элементов в подмножествах $\Delta_{i}$ \cite[с.~ 481-485]{max}.
		
		
\end{document}